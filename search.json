[{"path":"https://mlverse.github.io/tft/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 tft authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mlverse.github.io/tft/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Falbel. Author, maintainer. RStudio. Copyright holder. Christophe Regouby. Author.","code":""},{"path":"https://mlverse.github.io/tft/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Falbel D, Regouby C (2022). tft: Implementation Temporal Fusion Transformer. R package version 0.0.0.9000, https://mlverse.github.io/tft/.","code":"@Manual{,   title = {tft: Implementation of Temporal Fusion Transformer},   author = {Daniel Falbel and Christophe Regouby},   year = {2022},   note = {R package version 0.0.0.9000},   url = {https://mlverse.github.io/tft/}, }"},{"path":"https://mlverse.github.io/tft/index.html","id":"tft","dir":"","previous_headings":"","what":"Implementation of Temporal Fusion Transformer","title":"Implementation of Temporal Fusion Transformer","text":"R implementation : tft: Temporal Fusion Transformer. code repository R port akeskiner/Temporal_Fusion_Transform PyTorch’s implementation using torch package.","code":""},{"path":"https://mlverse.github.io/tft/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Implementation of Temporal Fusion Transformer","text":"can install development version {tft} GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"mlverse/tft\")"},{"path":"https://mlverse.github.io/tft/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Implementation of Temporal Fusion Transformer","text":"","code":"library(tft) library(rsample) suppressMessages(library(recipes)) suppressMessages(library(yardstick)) suppressMessages(library(tsibble)) set.seed(1)  data(\"vic_elec\", package = \"tsibbledata\") vic_elec <- vic_elec %>%    mutate(Location = as.factor(\"Victoria\"))  str(vic_elec) #> tbl_ts [52,608 × 6] (S3: tbl_ts/tbl_df/tbl/data.frame) #>  $ Time       : POSIXct[1:52608], format: \"2012-01-01 00:00:00\" \"2012-01-01 00:30:00\" ... #>  $ Demand     : num [1:52608] 4383 4263 4049 3878 4036 ... #>  $ Temperature: num [1:52608] 21.4 21.1 20.7 20.6 20.4 ... #>  $ Date       : Date[1:52608], format: \"2012-01-01\" \"2012-01-01\" ... #>  $ Holiday    : logi [1:52608] TRUE TRUE TRUE TRUE TRUE TRUE ... #>  $ Location   : Factor w/ 1 level \"Victoria\": 1 1 1 1 1 1 1 1 1 1 ... #>  - attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame) #>   ..$ .rows: list<int> [1:1]  #>   .. ..$ : int [1:52608] 1 2 3 4 5 6 7 8 9 10 ... #>   .. ..@ ptype: int(0)  #>  - attr(*, \"index\")= chr \"Time\" #>   ..- attr(*, \"ordered\")= logi TRUE #>  - attr(*, \"index2\")= chr \"Time\" #>  - attr(*, \"interval\")= interval [1:1] 30m #>   ..@ .regular: logi TRUE vic_elec_split <- initial_time_split(vic_elec, prop=3/4, lag=96)    vic_elec_train <- training(vic_elec_split) vic_elec_test <- testing(vic_elec_split)  rec <- recipe(Demand ~ ., data = vic_elec_train) %>%   update_role(Date, new_role=\"id\") %>%   update_role(Time, new_role=\"time\") %>%   update_role(Temperature, new_role=\"observed_input\") %>%   update_role(Holiday, new_role=\"known_input\") %>%   update_role(Location, new_role=\"static_input\") %>%   step_normalize(all_numeric(), -all_outcomes())   fit <- tft_fit(rec, vic_elec_train, epochs = 100, batch_size=100, total_time_steps=12, num_encoder_steps=10, verbose=TRUE)  yhat <- predict(fit, rec, vic_elec_test)"},{"path":"https://mlverse.github.io/tft/reference/batch_data.html","id":null,"dir":"Reference","previous_headings":"","what":"conditionning input data into tensors according to tft variable roles — batch_data","title":"conditionning input data into tensors according to tft variable roles — batch_data","text":"conditionning input data tensors according tft variable roles","code":""},{"path":"https://mlverse.github.io/tft/reference/batch_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"conditionning input data into tensors according to tft variable roles — batch_data","text":"","code":"batch_data(recipe, df, total_time_steps = 12, device)"},{"path":"https://mlverse.github.io/tft/reference/batch_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"conditionning input data into tensors according to tft variable roles — batch_data","text":"recipe recipe affecting tft roles df df data frame total_time_steps time_step value (default 48) device device use training. cpu cuda. default (auto) uses cuda available, otherwise uses cpu.","code":""},{"path":"https://mlverse.github.io/tft/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://mlverse.github.io/tft/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://mlverse.github.io/tft/reference/tft_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Configuration for Tft models — tft_config","title":"Configuration for Tft models — tft_config","text":"Configuration Tft models","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configuration for Tft models — tft_config","text":"","code":"tft_config(   batch_size = 256,   clip_value = NULL,   loss = \"quantile_loss\",   epochs = 5,   drop_last = FALSE,   total_time_steps = NULL,   num_encoder_steps = NULL,   quantiles = list(0.5),   training_tau = 0.3,   virtual_batch_size = 256,   valid_split = 0,   learn_rate = 0.02,   optimizer = \"adam\",   lr_scheduler = NULL,   lr_decay = 0.1,   step_size = 30,   checkpoint_epochs = 10,   cat_emb_dim = 1,   hidden_layer_size = 160,   dropout_rate = 0.3,   stack_size = 3,   num_heads = 1,   verbose = FALSE,   device = \"auto\" )"},{"path":"https://mlverse.github.io/tft/reference/tft_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configuration for Tft models — tft_config","text":"batch_size (int) Number examples per batch, large batch sizes recommended. (default: 1024) clip_value float given clip gradient clip_value. Pass NULL (default) clip. loss (character function) Loss function training within \"quantile_loss\", \"pinball_loss\", \"rmsse_loss\", \"smape_loss\" (default quantile_loss) epochs (int) Number training epochs. drop_last (bool) Whether drop last batch complete training total_time_steps (int) Size look-back time window + forecast horizon steps. width Temporal fusion decoder N. num_encoder_steps (int) Size look-back time window steps. size LSTM encoder. quantiles (list) list quantiles forcasts used quantile loss. (default = list(0.5)). training_tau (float) training_tau value used pinball loss. (default = 0.3). virtual_batch_size (int) Size mini batches used Batch Normalization (default=256) valid_split (float) fraction dataset used validation. learn_rate initial learning rate optimizer. optimizer optimization method. currently 'adam' supported, can also pass torch optimizer function. lr_scheduler NULL, (default) learning rate decay used. step decays learning rate lr_decay every step_size epochs. can also torch::lr_scheduler function takes optimizer parameter. step method called per epoch. lr_decay multiplies initial learning rate lr_decay every step_size epochs. Unused lr_scheduler torch::lr_scheduler NULL. step_size number epoch modifying learning rate lr_decay. Unused lr_scheduler torch::lr_scheduler NULL. checkpoint_epochs checkpoint model weights architecture every checkpoint_epochs. (default 10). may cause large memory usage. Use 0 disable checkpoints. cat_emb_dim (int list) Embedding size categorial features, broadcasted categorical feature, per categorical feature list size categorical features  (default=1) hidden_layer_size (int)size Internal state layer (default=160). dropout_rate dropout rate applied nn block (default=0.3) stack_size (int) Number self-attention layers apply (default=3). Use 1 basic TFT. num_heads (int) number interpretable multi-attention head (default=1) verbose (bool) wether print progress loss values training. device device use training. cpu cuda. default (auto) uses cuda`` available, otherwise uses cpu`.","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configuration for Tft models — tft_config","text":"named list hyperparameters TabNet implementation.","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Fusion Transformer model — tft_fit","title":"Temporal Fusion Transformer model — tft_fit","text":"Fits Temporal Fusion Transformer Interpretable Multi-horizon Time Series Forecasting model","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Fusion Transformer model — tft_fit","text":"","code":"tft_fit(x, ...)  # S3 method for default tft_fit(x, ...)  # S3 method for recipe tft_fit(x, df, tft_model = NULL, ..., from_epoch = NULL)"},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporal Fusion Transformer model — tft_fit","text":"x recipe specifying set preprocessing steps created recipes::recipe(). predictor data standardized (e.g. centered scaled). model treats categorical predictors internally thus, need make treatment. ... Model hyperparameters. See tft_config() list possible hyperparameters. df data frame containing predictors outcome. tft_model previously fitted TFT model object continue fitting . NULL (default) brand new model initialized. from_epoch tft_model provided, restore network weights specific epoch. Default last available checkpoint restored model, last epoch -memory model.","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Temporal Fusion Transformer model — tft_fit","text":"TFT model object. can used serialization, predictions, fitting.","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"fitting-a-pre-trained-model","dir":"Reference","previous_headings":"","what":"Fitting a pre-trained model","title":"Temporal Fusion Transformer model — tft_fit","text":"providing parent tft_model parameter, model fitting resumes model weights following epoch: last fitted epoch model already torch context Last model checkpoint epoch model loaded file epoch related checkpoint matching preceding from_epoch value provided model fitting metrics append top parent metrics returned TFT model.","code":""},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"threading","dir":"Reference","previous_headings":"","what":"Threading","title":"Temporal Fusion Transformer model — tft_fit","text":"TFT uses torch backend computation torch uses available threads default. can control number threads used torch :","code":"torch::torch_set_num_threads(1) torch::torch_set_num_interop_threads(1)"},{"path":"https://mlverse.github.io/tft/reference/tft_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Temporal Fusion Transformer model — tft_fit","text":"","code":"if (torch::torch_is_installed() && FALSE) { data(\"vic_elec\", package = \"tsibbledata\") library(recipes) rec <- recipe(Demand ~ ., data = vic_elec) %>%   update_role(Date, new_role=\"id\") %>%   update_role(Time, new_role=\"time\") %>%   update_role(Temperature, new_role=\"observed_input\") %>%   update_role(Holiday, new_role=\"known_input\") fit <- tft_fit(rec, df = vic_elec, epochs = 1, total_time_steps=10, num_encoder_steps=7) }"},{"path":"https://mlverse.github.io/tft/news/index.html","id":"tft-0009000","dir":"Changelog","previous_headings":"","what":"tft 0.0.0.9000","title":"tft 0.0.0.9000","text":"remove constraint development versions torch:: recipe:: Added NEWS.md file track changes package.","code":""}]
